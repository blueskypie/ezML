% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ML.funs.r
\name{ml.run}
\alias{ml.run}
\title{run machine learning models}
\usage{
ml.run(
  allDF = NULL,
  trainDF = NULL,
  testDF = NULL,
  preProc = c("center", "scale", "nzv"),
  classCol,
  posLabel = NULL,
  negLabel = NULL,
  cMethod,
  packName = NULL,
  trainInd = NULL,
  dummyEncoding = cMethod \%in\% c("xgbTree", "svmRadial"),
  prior.nzvCondX = NULL,
  prior.impute = c(NA, "missForest", "KNN"),
  oDir = ".",
  trainCtrlParas = NULL,
  trainParas = NULL
)
}
\arguments{
\item{allDF}{NULL, data.frame; the data.frame of all the data where rows are
observations and columns are features; training and testing will be sampled
at the ratio of 75 vs 25.}

\item{trainDF}{NULL, data.frame; the data.frame of the training data}

\item{testDF}{NULL, data.frame; the data.frame of the testing data}

\item{preProc}{\code{c("center", "scale",'nzv')}, char vector; preprocessing options; see \code{\link[caret:preProcess]{caret::preProcess()}}}

\item{classCol}{char; the column name of the class, i.e. the target variable}

\item{posLabel, negLabel}{NULL, char; for classification only; the positive and negative labels for two-class classification}

\item{cMethod}{char; the name of the ML method}

\item{packName}{NULL, char; the name of the package containing the ML method.
It's not needed if the \code{cMethod} is among those preset in \link{method2package}}

\item{trainInd}{NULL, list of integers; the indices of the resampling iteration in CV; see \code{index} in \code{\link[caret:trainControl]{caret::trainControl()}}}

\item{dummyEncoding}{\code{cMethod \%in\% c('xgbTree','svmRadial')},logical; is dummy variable
encoding needed on the combined training and testing datasets before training?
Although \code{caret::train()} handles dummy variable encoding automatically during CV and
prediction on testing. It's possible that there are levels in the holdout or
testing samples that are absent from the training data, causing errors.}

\item{prior.nzvCondX}{NULL, char; remove NZV and conditionalX variables before training.
This can be used only if \code{caret::train} is not used in the model building.
Following are the available options:
\itemize{
\item \code{NULL}: do not remove before training
\item one of \verb{n, nx}:
\itemize{
\item n: NZV
\item nx: NZV and ConditionalX
if detected, removed variables are written to files \code{nzv.csv} and \code{conditionalX.txt}
}
}}

\item{prior.impute}{\code{c(NA,'missForest','KNN')}, char; impute missing values before training.
This can be used only if \code{caret::train} is not used in the model building.
Following are the available options:
\itemize{
\item \code{NA}: do not impute before training
\item one of \verb{missForest, KNN}: impute using \code{\link[missForest:missForest]{missForest::missForest()}} or \code{\link[DMwR2:knnImputation]{DMwR2::knnImputation()}}
}}

\item{oDir}{NULL, char; output directory}

\item{trainCtrlParas}{NULL, list; parameters for train control in \code{\link[caret:trainControl]{caret::trainControl()}}.
Here are its default values if \code{method} is not \code{oob}:
\itemize{
\item \code{savePredictions = 'final'},
\item if \code{trainInd} is provided, \code{index = trainInd}
\item otherwise \verb{method = "repeatedcv",repeats = 3,number = 10}, which has the best bias-variance balance based on this
\href{http://appliedpredictivemodeling.com/blog/2014/11/27/vpuig01pqbklmi72b8lcl3ij5hj2qm}{post}
from Max Kuhn, the author of \code{caret}.
}

additional defaults for classification:
\itemize{
\item \code{classProbs = TRUE},
\item \code{summaryFunction = twoClassSummary} or \code{multiClassSummary} depending on the class column
}}

\item{trainParas}{NULL, list; parameters for train in \code{\link[caret:train]{caret::train()}}.
Here are its default values:
\itemize{
\item \code{tuneLength = 10}

\code{tuneLength} is the number of levels for each tuning parameters that should
be generated by train. If trainControl has the option \code{search = "random"}, this
is the maximum number of tuning parameter combinations that will be generated
by the random search. if \code{search = "grid"} which is the default, this is the maximum number of levels
of each tuning parameter. The actual number of levels used may be less than tuneLength.
additional defaults for classification:
\item \code{metric = "ROC"} or \code{'Accuracy'} depending on the class column
}}
}
\value{
a list of the following items
\itemize{
\item \code{imp}: data.frame, variable importance scores
\item \code{trainStats}: a named numerical vector containing AUC and various stats
in the confusion table from the prediction on the training dataset.
\item \code{testStats}: similar, but from the testing dataset.
}
}
\description{
This function compiles tools in \code{caret} packages to provide a machine learning (ML)
interface where users only need to provide the name of the ML method and the rest
are handled automatically.
}
\details{
The capabilities of this function include the following:
\itemize{
\item preprocessing of the whole dataset
\itemize{
\item dummy variable encoding
\item modification to the whole dataset before training, including
\itemize{
\item imputation using \code{missForest} and \code{KNN}
\item detection and removal of \verb{Near Zero Variance (NZV)} and \code{ConditionalX} variables
Note that these modifications apply only in situations where \code{caret::train}
is not used in the model building; for example

✦ ML method is \code{glm} where no cross validation (CV) is needed due to the absence of hyper-parameters

✦ ML method is \code{randomForest} where the training is done using \code{\link[randomForest:tuneRF]{randomForest::tuneRF()}}
}

Otherwise, those modification should be specified in \code{preProc} to avoid
information leakage during training. Note that \code{missForest} is not available
in \code{preProc}.
}
\item training and testing
\item the output of various files of the training and testing stats:
\itemize{
\item \emph{nzv.csv}: columns removed due to NZV and the details of each columns
\item \emph{train.all.stats.rds}: stats of the prediction of training model on the training dataset
\item \emph{train.cv.pdf}: curve of prediction performance during CV for upto four tuning parameters
\item \emph{train.model.rds}
\item \emph{train.prediction.csv}: prediction for each subject in training set
\item \emph{train.var.imp.pdf}: plot of variable importance
\item \emph{train.var.imp.csv}: importance score of each variable
\item \emph{test.all.stats.rds}
\item \emph{test.prediction.csv}
\item \emph{$cMethod.image.rdata}: running environment
for classification
\item \emph{train.confusion.table.txt}
\item \emph{train.prediction.hist.pdf}: prediction histogram
\item \emph{train.roc.pdf}: ROC curve
\item \emph{train.MDS12.png, train.MDS13.png, train.MDS23.png}: MDS plots of subjects if trained via \code{randomForest}
\item \emph{test.confusion.table.txt}
\item \emph{test.prediction.hist.pdf}
\item \emph{test.roc.pdf}
\item \emph{test.MDS12.png, test.MDS13.png, test.MDS23.png}
\item \emph{conditionalX.txt}: columns removed due to conditionalX
for regression
\item \emph{train.pred.cor.png}: scatter plot of raw and predicted values
\item \emph{train.pred.residual.png}: various diagnostic residual plots
\item \emph{test.pred.cor.png}: scatter plot of raw and predicted values
\item \emph{test.pred.residual.png}: plots of raw and standardized residuals
}
\item errors are handled to avoid crush.
}
}
\examples{
# see https://blueskypie.github.io/ezML/articles/ezML-intro.html
}
