[{"path":"/articles/ezML-intro.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"ezML-intro","text":"package makes running machine models supported caret package easy. Basically users need provide name ML method rest handled automatically. 20 output files produced covering various stages process, including preprocessing, training, testing, inference. See documentation ml.run() details examples .","code":""},{"path":[]},{"path":"/articles/ezML-intro.html","id":"dataset","dir":"Articles","previous_headings":"Examples","what":"dataset","title":"ezML-intro","text":"Let’s use titanic_train data contains columns type numerical categorical two three groups. First six rows titanic_train table listed information 891 passengers Titanic; row passenger rows contains missing values. meanings columns: Survived: binary (Y/N), passenger survive? Pclass: ordinal factor (c1, c2, c3), Ticket class (1 = upper, 2 = middle, 3 = lower) SibSp: integer, Number siblings spouses aboard Parch: integer, Number parents children aboard Fare: numeric, Passenger fare (British pounds) C = Cherbourg Q = Queenstown S = Southampton","code":"library(ezML) library(caret)  if (!requireNamespace(\"titanic\", quietly = TRUE)){   install.packages('titanic') } data(\"titanic_train\",package = 'titanic')  `%>%` = magrittr::`%>%` df1 = dplyr::mutate(titanic_train,            Survived = c('N','Y')[Survived+1],            Pclass = paste0('c',Pclass) %>% as.factor) %>%   dplyr::select(-c(PassengerId,Name,Ticket,Cabin))    knitr::kable(df1[1:6,],caption = 'First six rows of titanic_train')"},{"path":[]},{"path":"/articles/ezML-intro.html","id":"random-forest-models","dir":"Articles","previous_headings":"Examples > Classification","what":"Random Forest models","title":"ezML-intro","text":"Delgado et. al tested 179 classifiers 121 datasets paper Need Hundreds Classifiers Solve Real World Classification Problems?, Random Forest (RF) came top. (study carried 2014, right XGBoost invented.) examples running RF models using ezML::ml.run function different values parameter cMethod.","code":""},{"path":"/articles/ezML-intro.html","id":"randomforest","dir":"Articles","previous_headings":"Examples > Classification > Random Forest models","what":"randomForest","title":"ezML-intro","text":"Please read docs ml.run. explanations two parameters: cMethod = 'randomForest' setting package caret::train used; instead, uses randomForest::tuneRF directly model selection fitting. prior.impute = 'missForest' randomForest::tuneRF handle missing value, prior imputation done using missForest::missForest. run, folder randomForest created current directory, output files went . ml.run returns list following items: imp: data.frame, variable importance score. trainStats: named numerical vector containing AUC various stats prediction training dataset. testStats: similar, testing dataset.","code":"re = ml.run(allDF = df1,           classCol = 'Survived',           posLabel = 'Y',           negLabel = 'N',           cMethod = 'randomForest',           prior.impute = 'missForest'          )"},{"path":"/articles/ezML-intro.html","id":"rf","dir":"Articles","previous_headings":"Examples > Classification > Random Forest models","what":"rf","title":"ezML-intro","text":"Explanations: cMethod = 'rf' Use implemented randomForest::randomForest caret::train. preProc = c('nzv','bagImpute') One advantage using caret::train pre-processing, e.g. imputation, can done cross validation (CV), prior, avoid information leak. preProc corresponds preProcess caret::train; near-zero variance (nzv) features removed, missing values imputed using bagged tree model. trainCtrlParas = list(method='oob') trainCtrlParas list containing parameters caret::trainControl controls resampling CV. OOB, instead CV, used evaluate model performance ROC default two-class prediction.","code":"re = ml.run(allDF = df1,           classCol = 'Survived',           posLabel = 'Y',           negLabel = 'N',           cMethod = 'rf',           preProc = c('nzv','bagImpute'),           trainCtrlParas = list(method='oob')          )"},{"path":"/articles/ezML-intro.html","id":"parrf-and-ranger","dir":"Articles","previous_headings":"Examples > Classification > Random Forest models","what":"parRF and ranger","title":"ezML-intro","text":"Delgado paper also concluded parRF, parallel implementation randomForest caret, achieved slightly higher accuracy rf, unexpected. faster implementation parallel random forest ranger::ranger written C++. using OOB, probably due bugs caret, model compute probabilities prediction, thus Accuracy, instead ROC, used metric model evaluation. importance parameter ranger::ranger. Using CV enables computation ROC; slower OOB. default trainCtrlParas 10-fold 3-repeat CV, best bias-variance balance based post Max Kuhn, author caret.","code":"cl <- parallel::makePSOCKcluster(parallel::detectCores() - 1) doParallel::registerDoParallel(cl) re = ml.run(allDF = df1,           classCol = 'Survived',           posLabel = 'Y',           negLabel = 'N',           cMethod = 'parRF'           preProc = c('nzv','bagImpute'),           trainCtrlParas = list(method='oob')           ) parallel::stopCluster(cl) foreach::registerDoSEQ() re = ml.run(allDF = df1,           classCol = 'Survived',           posLabel = 'Y',           negLabel = 'N',           preProc = c('nzv','bagImpute'),           trainCtrlParas = list(method='oob',classProbs = TRUE),           cMethod = 'ranger',           trainParas = list(importance = \"impurity\")) re = ml.run(allDF = df1,           classCol = 'Survived',           posLabel = 'Y',           negLabel = 'N',           preProc = c('nzv','bagImpute'),           cMethod = 'ranger',           trainParas = list(importance = \"impurity\"))"},{"path":"/articles/ezML-intro.html","id":"svmradial","dir":"Articles","previous_headings":"Examples > Classification","what":"svmRadial","title":"ezML-intro","text":"SVM classifiers ranked second Delgado paper. one example. Note target variable Pclass contains three levels, data standardization (centering scaling) carried since RBF kernel involves computation distance high dimensional spaces standardization equalizes weight predictor computation.","code":"re = ml.run(allDF = df1,           classCol = 'Pclass',           cMethod = 'svmRadial',           preProc = c(\"center\", \"scale\",'nzv','knnImpute'))"},{"path":"/articles/ezML-intro.html","id":"glmnet","dir":"Articles","previous_headings":"Examples > Classification","what":"glmnet","title":"ezML-intro","text":"Elastic Net another fast accurate model. Since fits penalized generalized linear models, standardization ensures fair penalization across features. Although glmnet::glmnet standardization default, ’s turned caret set explicitly identical standardization training set can applied holdout CV prevent information leak.","code":"re = ml.run(allDF = df1,           classCol = 'Survived',           posLabel = 'Y',           negLabel = 'N',           preProc = c(\"center\", \"scale\",'nzv','knnImpute'),           cMethod = 'glmnet')"},{"path":"/articles/ezML-intro.html","id":"glm","dir":"Articles","previous_headings":"Examples > Classification","what":"glm","title":"ezML-intro","text":"Logistic regression simple often surprisingly good. hyper-parameters caret, checked caret::getModelInfo(\"glm\",regex = F)$parameters$parameter. Therefore, CV skipped.","code":"re = ml.run(allDF = df1,           classCol = 'Survived',           posLabel = 'Y',           negLabel = 'N',           cMethod = 'glm',           trainCtrlParas = list(method = 'none'),           trainParas = list(family = \"binomial\"))"},{"path":"/articles/ezML-intro.html","id":"xgbtree","dir":"Articles","previous_headings":"Examples > Classification","what":"xgbTree","title":"ezML-intro","text":"Gradient Boosting Machines often greater accuracy RFs, esp. tuning. addition, can handle missing values.","code":"xgbGrid <- expand.grid(   nrounds = seq.int(50,100,10),   max_depth = 6,   colsample_bytree = 1,   eta = 0.1,   gamma = 0,   min_child_weight = 1,   subsample = 1 )   re = ml.run(allDF = df1,           classCol = 'Survived',           posLabel = 'Y',           negLabel = 'N',           preProc = 'nzv',           cMethod = 'xgbTree',           trainParas = list(tuneGrid = xgbGrid))"},{"path":[]},{"path":[]},{"path":"/articles/ezML-intro.html","id":"randomforest-1","dir":"Articles","previous_headings":"Examples > regression > Random Forest models","what":"randomForest","title":"ezML-intro","text":"target variable numeric. output folder randomForest saved folder regr.","code":"re = ml.run(allDF = df1,           classCol = 'Fare',           prior.impute = 'missForest',           cMethod = 'randomForest',           oDir = 'regr')"},{"path":"/articles/ezML-intro.html","id":"rf-1","dir":"Articles","previous_headings":"Examples > regression > Random Forest models","what":"rf","title":"ezML-intro","text":"","code":"re = ml.run(allDF = df1,           classCol = 'Fare',           preProc = c('nzv','bagImpute'),           trainCtrlParas = list(method='oob'),           cMethod = 'rf',           oDir = 'regr')"},{"path":"/articles/ezML-intro.html","id":"glmnet-1","dir":"Articles","previous_headings":"Examples > regression","what":"glmnet","title":"ezML-intro","text":"","code":"re = ml.run(allDF = df1,           classCol = 'Fare',           preProc = c(\"center\", \"scale\",'nzv','knnImpute'),           cMethod = 'glmnet',           oDir = 'regr')"},{"path":"/articles/ezML-intro.html","id":"svmradial-1","dir":"Articles","previous_headings":"Examples > regression","what":"svmRadial","title":"ezML-intro","text":"","code":"re = ml.run(allDF = df1,           classCol = 'Fare',           preProc = c(\"center\", \"scale\",'nzv','knnImpute'),           cMethod = 'svmRadial',           oDir = 'regr')"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Jike Cui. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Cui J (2025). ezML: Plug-n-Play Machine Learning Wrapper. R package version 0.1.0, https://github.com/blueskypie/ezML.","code":"@Manual{,   title = {ezML: A Plug-n-Play Machine Learning Wrapper},   author = {Jike Cui},   year = {2025},   note = {R package version 0.1.0},   url = {https://github.com/blueskypie/ezML}, }"},{"path":"/index.html","id":"ezml","dir":"","previous_headings":"","what":"A Plug-n-Play Machine Learning Wrapper","title":"A Plug-n-Play Machine Learning Wrapper","text":"package makes running machine models supported caret package easy. Basically users need provide name ML method rest handled automatically. 20 output files produced covering various stages process, including preprocessing, training, testing, inference. See manual examples Since just toy, plan submit CRAN. please install github directly: devtools::install_github(\"blueskypie/ezML\")","code":""},{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 ezML authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"/reference/df.matched.html","id":null,"dir":"Reference","previous_headings":"","what":"check the unique values of non-numerical columns of two data.frames — df.matched","title":"check the unique values of non-numerical columns of two data.frames — df.matched","text":"Two data frames matched unique values non-numerical columns . model trained one data frame can predict response .","code":""},{"path":"/reference/df.matched.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"check the unique values of non-numerical columns of two data.frames — df.matched","text":"","code":"df.matched(df1, df2)"},{"path":"/reference/df.matched.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"check the unique values of non-numerical columns of two data.frames — df.matched","text":"df1, df2 two data frames identical column names orders","code":""},{"path":"/reference/df.matched.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"check the unique values of non-numerical columns of two data.frames — df.matched","text":"matrix following columns; NULL means matched. column: column names mis-match df1-df2: additional unique values df1 df2-df1: additional unique values df1","code":""},{"path":"/reference/df.matched.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"check the unique values of non-numerical columns of two data.frames — df.matched","text":"","code":"# none"},{"path":"/reference/df.preprocess.html","id":null,"dir":"Reference","previous_headings":"","what":"preprocessing before training — df.preprocess","title":"preprocessing before training — df.preprocess","text":"options preprocessing: dummy variable encoding modification whole dataset training, including imputation using missForest KNN detection removal Near Zero Variance (NZV) ConditionalX variables Note modifications apply situations caret::train used model building; example ML method glm cross validation (CV) needed due absence hyper-parameters ML method randomForest training done using randomForest::tuneRF() Otherwise, modification specified caret::train(preProc) avoid information leakage training. Note missForest available preProc.","code":""},{"path":"/reference/df.preprocess.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"preprocessing before training — df.preprocess","text":"","code":"df.preprocess(   allDF = NULL,   trainDF = NULL,   testDF = NULL,   prior.nzvCondX = c(\"n\", \"nx\"),   dummyEncoding = FALSE,   prior.impute = c(NA, \"missForest\", \"KNN\"),   classCol,   posLabel = NULL,   negLabel = NULL,   oDir = NULL )"},{"path":"/reference/df.preprocess.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"preprocessing before training — df.preprocess","text":"allDF NULL, data.frame; data.frame data trainDF NULL, data.frame; data.frame training data testDF NULL, data.frame; data.frame testing data prior.nzvCondX NULL, char; remove NZV conditionalX variables training. can used caret::train used model building. Following available options: * NULL: remove training * one n, nx: + n: NZV + nx: NZV ConditionalX detected, removed variables written files nzv.csv conditionalX.txt dummyEncoding cMethod %% c('xgbTree','svmRadial'),logical; dummy variable encoding needed training; prior.impute c(NA,'missForest','KNN'), char; impute missing values training. can used caret::train used model building. Following available options: * NA: impute training * one missForest, KNN: impute using missForest::missForest() DMwR2::knnImputation() classCol char; column name class, .e. response variable posLabel, negLabel NULL, char; positive negative labels two-class classification oDir NULL, char; output directory","code":""},{"path":"/reference/df.preprocess.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"preprocessing before training — df.preprocess","text":"list containing trainDF, testDF, allDF post processing.","code":""},{"path":"/reference/df.preprocess.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"preprocessing before training — df.preprocess","text":"","code":"# none"},{"path":"/reference/df.reset.html","id":null,"dir":"Reference","previous_headings":"","what":"reset Inf and char in a data.frame — df.reset","title":"reset Inf and char in a data.frame — df.reset","text":"Inf set NA, char columns factor","code":""},{"path":"/reference/df.reset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"reset Inf and char in a data.frame — df.reset","text":"","code":"df.reset(df, charAsFactor = TRUE)"},{"path":"/reference/df.reset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"reset Inf and char in a data.frame — df.reset","text":"df data.frame charAsFactor TRUE, logical; set char columns factor","code":""},{"path":"/reference/df.reset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"reset Inf and char in a data.frame — df.reset","text":"modified data.frame","code":""},{"path":"/reference/df.reset.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"reset Inf and char in a data.frame — df.reset","text":"","code":"# none"},{"path":"/reference/draw.classic.MDS.html","id":null,"dir":"Reference","previous_headings":"","what":"draw classic MDS plots — draw.classic.MDS","title":"draw classic MDS plots — draw.classic.MDS","text":"draw classic MDS plots","code":""},{"path":"/reference/draw.classic.MDS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"draw classic MDS plots — draw.classic.MDS","text":"","code":"draw.classic.MDS(   phenoDF,   fMat = NULL,   distMat = NULL,   PC = 1:3,   figMain = NULL,   ofPre = NULL,   colorCol = \"black\",   doReturn = TRUE,   ... )"},{"path":"/reference/draw.classic.MDS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"draw classic MDS plots — draw.classic.MDS","text":"phenoDF data.frame; row: subject; column: features used color plot fMat NULL, matrix; row: subjects order phenoDF; column: features distMat distance matrix; subjects order phenoDF; PC 1:3, integer vector; dimensions plot figMain NULL, char; title plots ofPre NULL, char; prefix output file name (including path) colorCol 'black',char; color subject plot; can also column phenoDF doReturn TRUE, logical; list ggplot objects returned? ... passed ggpubr::ggscatter","code":""},{"path":"/reference/draw.classic.MDS.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"draw classic MDS plots — draw.classic.MDS","text":"list ggplot objects doReturn = TRUE","code":""},{"path":"/reference/draw.classic.MDS.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"draw classic MDS plots — draw.classic.MDS","text":"","code":"# none"},{"path":"/reference/dummy.encode.html","id":null,"dir":"Reference","previous_headings":"","what":"convert categorical columns into numerical ones of full rank — dummy.encode","title":"convert categorical columns into numerical ones of full rank — dummy.encode","text":"following: nominal character variable n categories, n-1 binary variables created nominal factor variable n levels m categories (m<=n), * useLevel=TRUE,  n-1 binary variables created * otherwise, m-1 binary variables created ordinal factor variable n levels m categories, * m equals 2,  one variable assuming equal distance among levels created * otherwise, n-1 variable assuming linear 2 n-1 degree polynomial relationship among levels created. cells NA kept.","code":""},{"path":"/reference/dummy.encode.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"convert categorical columns into numerical ones of full rank — dummy.encode","text":"","code":"dummy.encode(df1, useLevel = FALSE, ...)"},{"path":"/reference/dummy.encode.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"convert categorical columns into numerical ones of full rank — dummy.encode","text":"df1 data frame; columns variables. useLevel FALSE, logical; FALSE, factor columns, unique values factor levels, instead actual levels present data. ... passed caret::dummyVars()","code":""},{"path":"/reference/dummy.encode.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"convert categorical columns into numerical ones of full rank — dummy.encode","text":"new data.frame categorical columns split dummy columns values 1 0.","code":""},{"path":"/reference/dummy.encode.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"convert categorical columns into numerical ones of full rank — dummy.encode","text":"","code":"k=matrix(1:10,nrow = 5) colnames(k)=c('n1','n2')  k=as.data.frame(k)  k$c5=LETTERS[1:5]  k$c2na=c('a','a',NA,'b','b')  k$f2na=factor(k$c2na)  k$f2naL2O=factor(k$f2na,ordered = TRUE)  k$f2naL3=factor(k$c2na,levels = c('a','b','c'))  k$f2naL3O=factor(k$f2naL3,ordered = TRUE)  k$f3na=factor(c('a','a',NA,'b','c'))  k$f3naL3O=factor(k$f3na,ordered = TRUE)  k$f3naL4O=factor(k$f3na,levels = c('a','b','c','d'),ordered = TRUE)  k$f3naL5O=factor(k$f3na,levels = c('a','b','c','d','e'),ordered = TRUE)  k #>   n1 n2 c5 c2na f2na f2naL2O f2naL3 f2naL3O f3na f3naL3O f3naL4O f3naL5O #> 1  1  6  A    a    a       a      a       a    a       a       a       a #> 2  2  7  B    a    a       a      a       a    a       a       a       a #> 3  3  8  C <NA> <NA>    <NA>   <NA>    <NA> <NA>    <NA>    <NA>    <NA> #> 4  4  9  D    b    b       b      b       b    b       b       b       b #> 5  5 10  E    b    b       b      b       b    c       c       c       c  dummy.encode(k) #>   n1 n2 c5B c5C c5D c5E c2nab f2na.b  f2naL2O.L f2naL3.b  f2naL3O.L f3na.b #> 1  1  6   0   0   0   0     0      0 -0.7071068        0 -0.7071068      0 #> 2  2  7   1   0   0   0     0      0 -0.7071068        0 -0.7071068      0 #> 3  3  8   0   1   0   0    NA     NA         NA       NA         NA     NA #> 4  4  9   0   0   1   0     1      1  0.7071068        1  0.7071068      1 #> 5  5 10   0   0   0   1     1      1  0.7071068        1  0.7071068      0 #>   f3na.c     f3naL3O.L  f3naL3O.Q  f3naL4O.L f3naL4O.Q  f3naL4O.C     f3naL5O.L #> 1      0 -7.071068e-01  0.4082483 -0.6708204       0.5 -0.2236068 -6.324555e-01 #> 2      0 -7.071068e-01  0.4082483 -0.6708204       0.5 -0.2236068 -6.324555e-01 #> 3     NA            NA         NA         NA        NA         NA            NA #> 4      0 -7.850462e-17 -0.8164966 -0.2236068      -0.5  0.6708204 -3.162278e-01 #> 5      1  7.071068e-01  0.4082483  0.2236068      -0.5 -0.6708204 -3.510833e-17 #>    f3naL5O.Q     f3naL5O.C  f3naL5O.4 #> 1  0.5345225 -3.162278e-01  0.1195229 #> 2  0.5345225 -3.162278e-01  0.1195229 #> 3         NA            NA         NA #> 4 -0.2672612  6.324555e-01 -0.4780914 #> 5 -0.5345225  1.755417e-16  0.7171372"},{"path":"/reference/list.concat.html","id":null,"dir":"Reference","previous_headings":"","what":"concatenate two lists — list.concat","title":"concatenate two lists — list.concat","text":"final list contains items lists; items left list share name right list covered latter.","code":""},{"path":"/reference/list.concat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"concatenate two lists — list.concat","text":"","code":"list.concat(list1, list2)"},{"path":"/reference/list.concat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"concatenate two lists — list.concat","text":"list1 left list list2 right list","code":""},{"path":"/reference/list.concat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"concatenate two lists — list.concat","text":"final list","code":""},{"path":"/reference/list.concat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"concatenate two lists — list.concat","text":"","code":"# none"},{"path":"/reference/method2package.html","id":null,"dir":"Reference","previous_headings":"","what":"get the name of R package for some frequently used methods — method2package","title":"get the name of R package for some frequently used methods — method2package","text":"preset method package pairs xgbTree = xgboost randomForest = randomForest rf = randomForest parRF = randomForest ranger = ranger glmnet = glmnet NaiveBayes = klaR svmRadial = e1071","code":""},{"path":"/reference/method2package.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"get the name of R package for some frequently used methods — method2package","text":"","code":"method2package(method)"},{"path":"/reference/method2package.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"get the name of R package for some frequently used methods — method2package","text":"method char; name method","code":""},{"path":"/reference/method2package.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"get the name of R package for some frequently used methods — method2package","text":"NA name package","code":""},{"path":"/reference/method2package.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"get the name of R package for some frequently used methods — method2package","text":"","code":"# none"},{"path":"/reference/ml.run.html","id":null,"dir":"Reference","previous_headings":"","what":"run machine learning models — ml.run","title":"run machine learning models — ml.run","text":"function compiles tools caret packages provide machine learning (ML) interface users need provide name ML method rest handled automatically, including: preprocessing whole dataset dummy variable encoding modification whole dataset training, including imputation using missForest KNN detection removal Near Zero Variance (NZV) ConditionalX variables Note modifications apply situations caret::train used model building; example ML method glm cross validation (CV) needed due absence hyper-parameters ML method randomForest training done using randomForest::tuneRF() Otherwise, modification specified preProc avoid information leakage training. Note missForest available preProc. training testing output various files training testing stats: nzv.csv: columns removed due NZV details columns train..stats.rds: stats prediction training model training dataset train.cv.pdf: curve prediction performance CV upto four tuning parameters train.model.rds train.prediction.csv: prediction subject training set train.var.imp.pdf: plot variable importance train.var.imp.csv: importance score variable test..stats.rds test.prediction.csv $cMethod.image.rdata: running environment classification train.confusion.table.txt train.prediction.hist.pdf: prediction histogram train.roc.pdf: ROC curve train.MDS12.png, train.MDS13.png, train.MDS23.png: MDS plots subjects trained via randomForest test.confusion.table.txt test.prediction.hist.pdf test.roc.pdf test.MDS12.png, test.MDS13.png, test.MDS23.png conditionalX.txt: columns removed due conditionalX regression train.pred.cor.png: scatter plot raw predicted values train.pred.residual.png: various diagnostic residual plots test.pred.cor.png: scatter plot raw predicted values test.pred.residual.png: plots raw standardized residuals error handled avoid crush","code":""},{"path":"/reference/ml.run.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"run machine learning models — ml.run","text":"","code":"ml.run(   allDF = NULL,   trainDF = NULL,   testDF = NULL,   preProc = c(\"center\", \"scale\", \"nzv\"),   classCol,   posLabel = NULL,   negLabel = NULL,   cMethod,   packName = NULL,   trainInd = NULL,   dummyEncoding = cMethod %in% c(\"xgbTree\", \"svmRadial\"),   prior.nzvCondX = NULL,   prior.impute = c(NA, \"missForest\", \"KNN\"),   oDir = \".\",   trainCtrlParas = NULL,   trainParas = NULL )"},{"path":"/reference/ml.run.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"run machine learning models — ml.run","text":"allDF NULL, data.frame; data.frame data rows observations columns features; training testing sampled ratio 75 vs 25. trainDF NULL, data.frame; data.frame training data testDF NULL, data.frame; data.frame testing data preProc c(\"center\", \"scale\",'nzv'), char vector; preprocessing options; see caret::preProcess() classCol char; column name class, .e. response variable posLabel, negLabel NULL, char; classification ; positive negative labels two-class classification cMethod char; name ML method packName NULL, char; name package containing ML method. needed cMethod among preset method2package trainInd NULL, list integers; indices resampling iteration CV; see index caret::trainControl() dummyEncoding cMethod %% c('xgbTree','svmRadial'),logical; dummy variable encoding needed combined training testing datasets training? Although caret::train() handles dummy variable encoding automatically CV prediction testing. possible levels holdout testing samples absent training data, causing error exit. prior.nzvCondX NULL, char; remove NZV conditionalX variables training. can used caret::train used model building. Following available options: * NULL: remove training * one n, nx: + n: NZV + nx: NZV ConditionalX detected, removed variables written files nzv.csv conditionalX.txt prior.impute c(NA,'missForest','KNN'), char; impute missing values training. can used caret::train used model building. Following available options: * NA: impute training * one missForest, KNN: impute using missForest::missForest() DMwR2::knnImputation() oDir NULL, char; output directory trainCtrlParas NULL, list; parameters train control caret::trainControl(). default values method oob: savePredictions = 'final', trainInd provided, index = trainInd otherwise method = \"repeatedcv\",repeats = 3,number = 10 CV method best bias-variance balance based post Max Kuhn, author caret. additional defaults classification: classProbs = TRUE, summaryFunction = twoClassSummary multiClassSummary depending class column trainParas NULL,list; parameters train caret::train(). default values: tuneLength = 10 tuneLength number levels tuning parameters generated train. trainControl option search = \"random\", maximum number tuning parameter combinations generated random search. search = \"grid\" default, maximum number levels tuning parameter. actual number levels used may less tuneLength. additional defaults classification: metric = \"ROC\" 'Accuracy' depending class column","code":""},{"path":"/reference/ml.run.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"run machine learning models — ml.run","text":"list following items * imp: data.frame, variable importance score * trainStats: named numerical vector containing AUC various stats confusion table prediction training dataset. * testStats: similar, testing dataset.","code":""},{"path":"/reference/ml.run.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"run machine learning models — ml.run","text":"","code":"# none"},{"path":"/reference/plotPredProbs.html","id":null,"dir":"Reference","previous_headings":"","what":"plot prediction probability histogram — plotPredProbs","title":"plot prediction probability histogram — plotPredProbs","text":"Produce back--back histogram prediction probability distributions positive negative labels","code":""},{"path":"/reference/plotPredProbs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"plot prediction probability histogram — plotPredProbs","text":"","code":"plotPredProbs(predProbs, trueLabels, posLabel)"},{"path":"/reference/plotPredProbs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"plot prediction probability histogram — plotPredProbs","text":"predProbs data.frame; one column data.frame column name positive label rownames subject IDs. trueLabels char; true class label posLabel char; positive label","code":""},{"path":"/reference/plotPredProbs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"plot prediction probability histogram — plotPredProbs","text":"none","code":""},{"path":"/reference/plotPredProbs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"plot prediction probability histogram — plotPredProbs","text":"","code":"#none"},{"path":"/reference/predStats.Categorical.html","id":null,"dir":"Reference","previous_headings":"","what":"compute the prediction stats of a model — predStats.Categorical","title":"compute the prediction stats of a model — predStats.Categorical","text":"function applies response variable type factor. computes prediction stats model write following files","code":""},{"path":"/reference/predStats.Categorical.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"compute the prediction stats of a model — predStats.Categorical","text":"","code":"predStats.Categorical(   mFit,   newData,   classInd,   posLabel = NULL,   ofPre = NULL,   predProbs = NULL,   nLevels = NULL,   ... )"},{"path":"/reference/predStats.Categorical.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"compute the prediction stats of a model — predStats.Categorical","text":"mFit model newData data.frame; new data classInd integer; index class column posLabel NULL, char; positive labels ofPre NULL, char; prefix output file predProbs NULL, data.frame; supplied prediction probability; column names levels response variable. nLevels NULL, number levels class column. ... passed caret::predict()","code":""},{"path":"/reference/predStats.Categorical.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"compute the prediction stats of a model — predStats.Categorical","text":"named numerical vector stats","code":""},{"path":"/reference/predStats.Categorical.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"compute the prediction stats of a model — predStats.Categorical","text":"","code":"# none"},{"path":"/reference/rm.NZV.condX.html","id":null,"dir":"Reference","previous_headings":"","what":"remove low variance/varying and conditionalX features — rm.NZV.condX","title":"remove low variance/varying and conditionalX features — rm.NZV.condX","text":"low varying features non-numeric features frequent value accounts >95% values. low variance conditionalX features defined caret::nearZeroVar() caret::checkConditionalX()","code":""},{"path":"/reference/rm.NZV.condX.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"remove low variance/varying and conditionalX features — rm.NZV.condX","text":"","code":"rm.NZV.condX(df1, y = NULL, ofPre = NULL)"},{"path":"/reference/rm.NZV.condX.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"remove low variance/varying and conditionalX features — rm.NZV.condX","text":"df1 data.frame; input y NULL, char; column name response variable ofPre NULL, char; prefix output file names: * conditionalX.txt * nzv.csv","code":""},{"path":"/reference/rm.NZV.condX.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"remove low variance/varying and conditionalX features — rm.NZV.condX","text":"integer vector; removed indices","code":""},{"path":"/reference/rm.NZV.condX.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"remove low variance/varying and conditionalX features — rm.NZV.condX","text":"","code":"#none"},{"path":"/reference/writePredStats_bestTunes.html","id":null,"dir":"Reference","previous_headings":"","what":"write the best-tune parameters to a file — writePredStats_bestTunes","title":"write the best-tune parameters to a file — writePredStats_bestTunes","text":"write best-tune parameters file","code":""},{"path":"/reference/writePredStats_bestTunes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"write the best-tune parameters to a file — writePredStats_bestTunes","text":"","code":"writePredStats_bestTunes(mFit, fileName, statTable, aucVal = NA)"},{"path":"/reference/writePredStats_bestTunes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"write the best-tune parameters to a file — writePredStats_bestTunes","text":"mFit final model fileName char; output file name statTable statistic table aucVal NA, numeric; prediction AUC","code":""},{"path":"/reference/writePredStats_bestTunes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"write the best-tune parameters to a file — writePredStats_bestTunes","text":"none","code":""},{"path":"/reference/writePredStats_bestTunes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"write the best-tune parameters to a file — writePredStats_bestTunes","text":"","code":"# none"},{"path":"/news/index.html","id":"ezml-010","dir":"Changelog","previous_headings":"","what":"ezML 0.1.0","title":"ezML 0.1.0","text":"Initial CRAN submission.","code":""}]
